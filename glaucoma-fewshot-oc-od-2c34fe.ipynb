{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3863247,"sourceType":"datasetVersion","datasetId":2296461},{"sourceId":6609775,"sourceType":"datasetVersion","datasetId":3814003},{"sourceId":11241504,"sourceType":"datasetVersion","datasetId":7023463},{"sourceId":11243819,"sourceType":"datasetVersion","datasetId":7025241},{"sourceId":328291,"sourceType":"modelInstanceVersion","modelInstanceId":275492,"modelId":296384}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# One-cell script: Proto-Attention U-Net few-shot with HybridLoss (CE + Dice + Align),\n# episodic training, tqdm.notebook, cosine LR, per-epoch loss prints, visualizations,\n# end-of-training plots and per-class metrics. Images: .jpg, Masks: .bmp\n\nimport os, math, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport kornia  # for GPU-accelerated CLAHE\n\n# --------------------- CLAHE Transform ---------------------\nclass Clahe(nn.Module):\n    \"\"\"\n    Contrast-Limited Adaptive Histogram Equalization (CLAHE) using Kornia.\n    \"\"\"\n    def __init__(self, clip_limit: float = 40.0, grid_size=(8, 8)):\n        super().__init__()\n        self.clip_limit = float(clip_limit)\n        self.grid_size = grid_size\n\n    def forward(self, img: torch.Tensor) -> torch.Tensor:\n        # img: (C, H, W), values in [0,1]\n        batch = img.unsqueeze(0)\n        # correct argument name: grid_size instead of tile_grid_size\n        eq = kornia.enhance.equalize_clahe(\n            batch,\n            clip_limit=self.clip_limit,\n            grid_size=self.grid_size\n        )\n        return eq.squeeze(0)\n\n    def __repr__(self):\n        return (f\"{self.__class__.__name__}(clip_limit={self.clip_limit}, \"\n                f\"grid_size={self.grid_size})\")\n\n# --------------------- Dataset ---------------------\nclass GlaucomaDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, img_size=128,\n                 clahe_clip=40.0, clahe_grid=(8,8)):\n        self.img_dir, self.mask_dir, self.img_size = img_dir, mask_dir, img_size\n        self.samples = sorted(f for f in os.listdir(img_dir) if f.lower().endswith('.jpg'))\n\n        base_transforms = [\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            Clahe(clip_limit=clahe_clip, grid_size=clahe_grid),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225])\n        ]\n        self.sup_tf = transforms.Compose(base_transforms)\n        self.qry_tf = transforms.Compose(base_transforms)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        fn = self.samples[idx]\n        img = Image.open(os.path.join(self.img_dir, fn)).convert('RGB')\n        mask_fn = fn.rsplit('.', 1)[0] + '.bmp'\n        mask = Image.open(os.path.join(self.mask_dir, mask_fn)).convert('L')\n        mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)\n        m = np.array(mask)\n        lbl = np.zeros_like(m, dtype=int)\n        if m.max() > 2:\n            lbl[m == 128] = 1\n            lbl[m == 255] = 2\n        else:\n            lbl = m\n        return img, torch.from_numpy(lbl).long()\n\n# -------------------- Model --------------------\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.block(x)\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__()\n        self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid())\n    def forward(self, x, g):\n        g1, x1 = self.W_g(g), self.W_x(x)\n        return x * self.psi(F.relu(g1 + x1, inplace=True))\n\nclass UpBlock(nn.Module):\n    def __init__(self, up_in, skip_ch, out_ch):\n        super().__init__()\n        self.up   = nn.ConvTranspose2d(up_in, out_ch, 2, stride=2)\n        self.att  = AttentionGate(out_ch, skip_ch, skip_ch // 2)\n        self.conv = ConvBlock(out_ch + skip_ch, out_ch)\n    def forward(self, x, skip):\n        xu = self.up(x)\n        sa = self.att(skip, xu)\n        return self.conv(torch.cat([xu, sa], dim=1))\n\nclass ProtoAttentionUNet(nn.Module):\n    def __init__(self, in_ch=3, num_classes=3, feat_dim=256):\n        super().__init__()\n        self.enc1 = ConvBlock(in_ch, 64)\n        self.enc2 = ConvBlock(64, 128)\n        self.enc3 = ConvBlock(128, 256)\n        self.pool = nn.MaxPool2d(2)\n        self.proj     = nn.Conv2d(256, feat_dim, 1)\n        self.sim_conv = nn.Conv2d(num_classes, feat_dim, 1)\n        self.up3  = UpBlock(feat_dim, 128, 128)\n        self.up2  = UpBlock(128, 64, 64)\n        self.final = nn.Conv2d(64, num_classes, 1)\n\n    def forward(self, s_imgs, s_masks, q_imgs):\n        # Support encoding & prototype computation\n        s1 = self.enc1(s_imgs)\n        s2 = self.enc2(self.pool(s1))\n        s3 = self.enc3(self.pool(s2))\n        s_proj = self.proj(s3)\n        m_ds = F.interpolate(s_masks.unsqueeze(1).float(),\n                             size=s_proj.shape[2:], mode='nearest').squeeze(1).long()\n        protos = []\n        for c in range(self.final.out_channels):\n            m = (m_ds == c).unsqueeze(1).float()\n            p = (s_proj * m).sum((0,2,3)) / (m.sum() + 1e-6)\n            protos.append(p)\n        prototypes = F.normalize(torch.stack(protos), dim=1)\n\n        # Query encoding & similarity map\n        q1 = self.enc1(q_imgs)\n        q2 = self.enc2(self.pool(q1))\n        q3 = self.enc3(self.pool(q2))\n        q_proj = self.proj(q3)\n        B, D, h, w = q_proj.shape\n        q_flat = q_proj.view(B, D, -1).permute(0, 2, 1)\n        qn = F.normalize(q_flat, dim=2)\n        sim = torch.matmul(qn, prototypes.t())\n        sim_map = sim.permute(0, 2, 1).view(B, self.final.out_channels, h, w)\n        sim_feat = self.sim_conv(sim_map)\n\n        # Decode with attention\n        x3 = self.up3(sim_feat, q2)\n        x2 = self.up2(x3, q1)\n        return self.final(x2), prototypes\n\n# --------------------- Losses ---------------------\nclass DiceLoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n    def forward(self, preds, targets):\n        C = preds.shape[1]\n        t = F.one_hot(targets, C).permute(0,3,1,2).float()\n        p = F.softmax(preds, 1)\n        inter = (p * t).sum((2,3))\n        union = p.sum((2,3)) + t.sum((2,3))\n        dice = (2*inter + self.eps) / (union + self.eps)\n        return 1 - dice.mean()\n\nclass PrototypeAlignmentLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, prototypes):\n        sim = F.cosine_similarity(prototypes.unsqueeze(1),\n                                  prototypes.unsqueeze(0), dim=-1)\n        return F.mse_loss(sim, torch.eye(sim.size(0), device=sim.device))\n\nclass HybridLoss(nn.Module):\n    def __init__(self, class_weights=[0.1, 0.43, 0.47], align_w=0.3):\n        super().__init__()\n        w = torch.tensor(class_weights)\n        self.register_buffer('ce_weight', w)\n        self.ce    = nn.CrossEntropyLoss(weight=self.ce_weight)\n        self.dice  = DiceLoss()\n        self.align = PrototypeAlignmentLoss()\n        self.align_w = align_w\n\n    def forward(self, preds, targets, prototypes):\n        l_ce = self.ce(preds, targets)\n        l_d  = self.dice(preds, targets)\n        l_a  = self.align(prototypes)\n        loss = l_ce + l_d + self.align_w * l_a\n        return loss, (l_ce.item(), l_d.item(), l_a.item())\n\n# ---------------- Episodic Trainer ----------------\nclass EpisodicTrainer:\n    def __init__(self, img_dir, mask_dir,\n                 img_size=128, num_support=3, num_query=5,\n                 episodes_per_epoch=200, epochs=30,\n                 lr=1e-3, wd=1e-5, device=None):\n        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.ds     = GlaucomaDataset(img_dir, mask_dir, img_size)\n        self.ns, self.nq      = num_support, num_query\n        self.ep_per, self.epochs = episodes_per_epoch, epochs\n\n        self.model = ProtoAttentionUNet().to(self.device)\n        self.opt   = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=wd)\n        self.sched = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt,\n                                                                 T_max=epochs,\n                                                                 eta_min=1e-5)\n        self.crit  = HybridLoss().to(self.device)\n\n        self.h_loss, self.h_score = [], []\n\n    def train(self):\n        for ep in range(1, self.epochs+1):\n            self.model.train()\n            tot_ce = tot_d = tot_a = tot_l = tot_sc = 0.0\n\n            for _ in tqdm(range(self.ep_per), desc=f\"Epoch {ep}\", leave=False):\n                idxs = np.arange(len(self.ds))\n                sel  = np.random.choice(idxs, self.ns + self.nq, replace=False)\n                sup = [self.ds[i] for i in sel[:self.ns]]\n                qry = [self.ds[i] for i in sel[self.ns:]]\n\n                s_x = torch.stack([self.ds.sup_tf(x[0]) for x in sup]).to(self.device)\n                s_m = torch.stack([x[1] for x in sup]).to(self.device)\n                q_x = torch.stack([self.ds.qry_tf(x[0]) for x in qry]).to(self.device)\n                q_m = torch.stack([x[1] for x in qry]).to(self.device)\n\n                preds, protos = self.model(s_x, s_m, q_x)\n                loss, (l_ce, l_d, l_a) = self.crit(preds, q_m, protos)\n\n                self.opt.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.opt.step()\n\n                tot_ce += l_ce\n                tot_d  += l_d\n                tot_a  += l_a\n                tot_l  += loss.item()\n                tot_sc += (1 - DiceLoss()(preds, q_m).item())\n\n            self.sched.step()\n\n            avg_ce = tot_ce / self.ep_per\n            avg_d  = tot_d  / self.ep_per\n            avg_a  = tot_a  / self.ep_per\n            avg_l  = tot_l  / self.ep_per\n            avg_sc = tot_sc / self.ep_per\n\n            self.h_loss.append(avg_l)\n            self.h_score.append(avg_sc)\n\n            print(f\"Epoch {ep}/{self.epochs} | CE={avg_ce:.4f} | DiceLoss={avg_d:.4f} | \"\n                  f\"Align={avg_a:.4f} | Total={avg_l:.4f} | DiceScore={avg_sc:.4f}\")\n\n            if ep % 2 == 0:\n                self._visualize(q_x[0], q_m[0], preds[0])\n\n        self._plot_curves()\n        self._eval_metrics()\n\n    def _visualize(self, img, gt, logit):\n        pred = logit.argmax(0).cpu().numpy()\n        im   = img.cpu().permute(1,2,0).numpy() * [0.229,0.224,0.225] + [0.485,0.456,0.406]\n        fig, ax = plt.subplots(1,3,figsize=(12,4))\n        ax[0].imshow(im);            ax[0].set_title(\"Query\"); ax[0].axis('off')\n        ax[1].imshow(gt.cpu(),cmap='gray'); ax[1].set_title(\"GT\");    ax[1].axis('off')\n        ax[2].imshow(pred,cmap='gray');     ax[2].set_title(\"Pred\");  ax[2].axis('off')\n        plt.show()\n\n    def _plot_curves(self):\n        epochs = range(1, self.epochs+1)\n        plt.figure(figsize=(12,4))\n        plt.subplot(1,2,1); plt.plot(epochs, self.h_loss,'-o');       plt.title(\"Loss\");        plt.xlabel(\"Epoch\")\n        plt.subplot(1,2,2); plt.plot(epochs, self.h_score,'-o');      plt.title(\"Dice Score\");  plt.xlabel(\"Epoch\")\n        plt.show()\n\n    def _eval_metrics(self, eval_eps=100):\n        C = self.model.final.out_channels\n        tp = np.zeros(C); fp = np.zeros(C); fn = np.zeros(C)\n        self.model.eval()\n        with torch.no_grad():\n            for _ in range(eval_eps):\n                idxs = np.arange(len(self.ds))\n                sel  = np.random.choice(idxs, self.ns+1, replace=False)\n                sup = [self.ds[i] for i in sel[:self.ns]]\n                qry = self.ds[sel[-1]]\n                s_x = torch.stack([self.ds.sup_tf(x[0]) for x in sup]).to(self.device)\n                s_m = torch.stack([x[1] for x in sup]).to(self.device)\n                q_img, q_gt = qry\n                q_x = self.ds.qry_tf(q_img).unsqueeze(0).to(self.device)\n\n                logits, _ = self.model(s_x, s_m, q_x)\n                pred = logits.argmax(1).squeeze(0).cpu().numpy()\n                gt   = q_gt.numpy()\n                for c in range(C):\n                    tp[c] += ((pred==c)&(gt==c)).sum()\n                    fp[c] += ((pred==c)&(gt!=c)).sum()\n                    fn[c] += ((pred!=c)&(gt==c)).sum()\n\n        print(\"Per-class Dice & IoU:\")\n        for c in range(C):\n            dice = 2*tp[c]/(2*tp[c]+fp[c]+fn[c]+1e-6)\n            iou  = tp[c]/(tp[c]+fp[c]+fn[c]+1e-6)\n            print(f\" Class {c}: Dice={dice:.4f}, IoU={iou:.4f}\")\n\n    def save(self, path='model_final.pth'):\n        torch.save(self.model.state_dict(), path)\n        print(\"Saved:\", path)\n\nif __name__ == '__main__':\n    IMG_DIR = '/kaggle/input/refuge2/REFUGE2/train/images'\n    MSK_DIR = '/kaggle/input/refuge2/REFUGE2/train/mask'\n    trainer = EpisodicTrainer(\n        IMG_DIR, MSK_DIR,\n        img_size=128,\n        num_support=3,\n        num_query=5,\n        episodes_per_epoch=50,\n        epochs=30,\n        lr=1e-3,\n        wd=1e-5\n    )\n    trainer.train()\n    trainer.save()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport kornia\n\n# re‐use your Clahe module definition from training\nclass Clahe(torch.nn.Module):\n    def __init__(self, clip_limit: float = 40.0, grid_size=(8, 8)):\n        super().__init__()\n        self.clip_limit = float(clip_limit)\n        self.grid_size = grid_size\n\n    def forward(self, img: torch.Tensor) -> torch.Tensor:\n        batch = img.unsqueeze(0)\n        eq = kornia.enhance.equalize_clahe(\n            batch,\n            clip_limit=self.clip_limit,\n            grid_size=self.grid_size\n        )\n        return eq.squeeze(0)\n\n# --- Paths & params ---\nIMG_DIR     = '/kaggle/input/refuge2/REFUGE2/test/images'\nMSK_DIR     = '/kaggle/input/refuge2/REFUGE2/test/mask'\nMODEL_PATH  = '/kaggle/working/model_final.pth'\nIMG_SIZE    = 128\nNUM_SUPPORT = 3\nNUM_EPISODES= 50\nDEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Deterministic test‐time transform (must match train) ---\ntf_test = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    Clahe(clip_limit=40.0, grid_size=(8,8)),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# --- Mask loader (REFUGE encoding: 0=BG,128=OD,255=OC) ---\ndef load_mask(path):\n    m = np.array(Image.open(path)\n                   .convert('L')\n                   .resize((IMG_SIZE, IMG_SIZE), Image.NEAREST))\n    lbl = np.zeros_like(m, dtype=int)\n    lbl[m == 128] = 1\n    lbl[m == 255] = 2\n    return lbl\n\n# --- Visualization helper ---\ndef color_mask(mask):\n    h, w = mask.shape\n    col = np.zeros((h, w, 3), float)\n    col[mask == 0] = [1,1,0]   # BG→yellow\n    col[mask == 1] = [0,1,0]   # OD→green\n    col[mask == 2] = [1,0,0]   # OC→red\n    return col\n\n# --- Gather files ---\nimg_files = sorted([\n    os.path.join(IMG_DIR, f) for f in os.listdir(IMG_DIR)\n    if f.lower().endswith(('.jpg','.png','.tif'))\n])\nmask_map = {\n    os.path.splitext(f)[0]: os.path.join(MSK_DIR, f)\n    for f in os.listdir(MSK_DIR)\n    if f.lower().endswith(('.bmp','.png','.jpg'))\n}\n\n# --- Load model ---\nmodel = ProtoAttentionUNet(in_ch=3, num_classes=3, feat_dim=256).to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\n\n# --- Metric functions ---\ndef dice_scores(pred, gt, eps=1e-6):\n    scores = []\n    for c in range(3):\n        p = (pred == c); g = (gt == c)\n        inter = (p & g).sum()\n        union = p.sum() + g.sum()\n        scores.append((2*inter + eps) / (union + eps))\n    return np.array(scores)\n\n# --- Confusion matrix & accumulator ---\nconf_mat = np.zeros((3,3), int)\nall_dices = []\n\n# --- Few‐shot episodes ---\nfor _ in tqdm(range(NUM_EPISODES), desc='Meta-Test REFUGE2'):\n    # sample support + query\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT + 1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    # support batch\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask = load_mask(mask_map[key])\n        s_imgs.append(tf_test(img))\n        s_msks.append(torch.from_numpy(mask).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    # query\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB')\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_test(q_pil).unsqueeze(0).to(DEVICE)\n\n    # inference\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n    pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    # record metrics\n    all_dices.append(dice_scores(pred, q_gt))\n    for i in range(3):\n        for j in range(3):\n            conf_mat[i, j] += int(((q_gt == i) & (pred == j)).sum())\n\n# --- Results ---\nall_dices = np.stack(all_dices)\nmean_d = all_dices.mean(axis=0)\nprint(f\"Mean per-class Dice: BG={mean_d[0]:.4f}, OD={mean_d[1]:.4f}, OC={mean_d[2]:.4f}\")\nprint(f\"Overall Mean Dice: {mean_d.mean():.4f}\\n\")\nprint(\"Confusion Matrix (rows=GT, cols=Pred):\\n\", conf_mat)\n\n# --- Plot confusion matrix ---\nplt.figure(figsize=(5,5))\nplt.imshow(conf_mat, cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\"); plt.ylabel(\"Ground Truth\")\nfor i in range(3):\n    for j in range(3):\n        plt.text(j, i, conf_mat[i, j], ha='center', va='center', color='red')\nplt.xticks([0,1,2], ['BG','OD','OC'])\nplt.yticks([0,1,2], ['BG','OD','OC'])\nplt.show()\n\n# --- Visualize a few episodes ---\nfor _ in range(5):\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT + 1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask = load_mask(mask_map[key])\n        s_imgs.append(tf_test(img))\n        s_msks.append(torch.from_numpy(mask).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB').resize((IMG_SIZE,IMG_SIZE))\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_test(q_pil).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n    pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    fig, ax = plt.subplots(1, 3, figsize=(12,4))\n    ax[0].imshow(q_pil);           ax[0].set_title(\"Query Image\"); ax[0].axis('off')\n    ax[1].imshow(color_mask(q_gt)); ax[1].set_title(\"GT Mask\");     ax[1].axis('off')\n    ax[2].imshow(color_mask(pred)); ax[2].set_title(\"Pred Mask\");   ax[2].axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:39:40.687533Z","iopub.execute_input":"2025-05-03T09:39:40.687872Z","iopub.status.idle":"2025-05-03T09:40:01.121248Z","shell.execute_reply.started":"2025-05-03T09:39:40.687833Z","shell.execute_reply":"2025-05-03T09:40:01.120458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport kornia  # GPU‐accelerated image ops\n\n# re‐use your Clahe module\nclass Clahe(torch.nn.Module):\n    def __init__(self, clip_limit: float = 40.0, grid_size=(8, 8)):\n        super().__init__()\n        self.clip_limit = float(clip_limit)\n        self.grid_size = grid_size\n\n    def forward(self, img: torch.Tensor) -> torch.Tensor:\n        batch = img.unsqueeze(0)\n        eq = kornia.enhance.equalize_clahe(\n            batch,\n            clip_limit=self.clip_limit,\n            grid_size=self.grid_size\n        )\n        return eq.squeeze(0)\n\n# --- Color mask for visualization ---\ndef color_mask(mask):\n    h,w = mask.shape\n    col = np.zeros((h,w,3),dtype=float)\n    col[mask==0] = [1,1,0]   # BG  → yellow\n    col[mask==1] = [0,1,0]   # OD  → green\n    col[mask==2] = [1,0,0]   # OC  → red\n    return col\n\n# --- Paths & params ---\nORIGA_IMG_DIR  = '/kaggle/input/glaucoma-datasets/REFUGE/train/Images'\nORIGA_MSK_DIR  = '/kaggle/input/glaucoma-datasets/REFUGE/train/Masks'\nMODEL_PATH     = '/kaggle/working/model_final.pth'\nIMG_SIZE       = 128\nNUM_SUPPORT    = 3\nNUM_EPISODES   = 50\nDEVICE         = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Transforms (with CLAHE) ---\ntf_img = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    Clahe(clip_limit=40.0, grid_size=(8,8)),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- Load mask (0,1,2 encoding) ---\ndef load_mask(path):\n    m = Image.open(path).convert('L')\n    m = m.resize((IMG_SIZE,IMG_SIZE), Image.NEAREST)\n    return np.array(m, dtype=int)\n\n# --- Gather file lists ---\nimg_files = sorted([\n    os.path.join(ORIGA_IMG_DIR,f)\n    for f in os.listdir(ORIGA_IMG_DIR) if f.lower().endswith('.jpg')\n])\nmask_map = {\n    os.path.splitext(f)[0]: os.path.join(ORIGA_MSK_DIR,f)\n    for f in os.listdir(ORIGA_MSK_DIR) if f.lower().endswith('.png')\n}\n\n# --- Load model ---\nmodel = ProtoAttentionUNet(in_ch=3, num_classes=3, feat_dim=256).to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\n\n# --- Dice metric ---\ndef dice_score(pred, gt, eps=1e-6):\n    scores = []\n    for c in range(3):\n        p = (pred==c)\n        g = (gt  ==c)\n        inter = (p & g).sum()\n        union = p.sum() + g.sum()\n        scores.append((2*inter+eps)/(union+eps))\n    return np.array(scores)\n\n# --- Confusion matrix accumulator ---\nconf_mat = np.zeros((3,3), dtype=int)\nall_scores = []\n\n# --- Meta-test episodes ---\nfor _ in tqdm(range(NUM_EPISODES), desc='Meta-Test'):\n    # sample support + query\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT+1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    # build support tensors\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img_pil = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask_arr = load_mask(mask_map[key])\n        s_imgs.append(tf_img(img_pil))\n        s_msks.append(torch.from_numpy(mask_arr).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    # load query\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB')\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_img(q_pil).unsqueeze(0).to(DEVICE)\n\n    # inference + channel‐swap fix\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n        logits = logits[:, [2,1,0], :, :]\n        pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    # record dice\n    sc = dice_score(pred, q_gt)\n    all_scores.append(sc)\n\n    # update confusion\n    for i in range(3):\n        for j in range(3):\n            conf_mat[i,j] += int(((q_gt==i) & (pred==j)).sum())\n\n# --- Report results ---\nall_scores = np.stack(all_scores)\nmean_dice = all_scores.mean(axis=0)\nprint(\"Mean per-class Dice:\")\nprint(f\"  BG: {mean_dice[0]:.4f}, OD: {mean_dice[1]:.4f}, OC: {mean_dice[2]:.4f}\")\nprint(f\"Overall Mean Dice: {mean_dice.mean():.4f}\\n\")\n\nprint(\"Confusion Matrix (rows=GT, cols=Pred):\")\nprint(conf_mat)\n\n# --- Plot confusion matrix ---\nplt.figure(figsize=(5,5))\nplt.imshow(conf_mat, cmap='Blues')\nplt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Ground Truth\")\nfor i in range(3):\n    for j in range(3):\n        plt.text(j, i, conf_mat[i,j], ha='center', va='center', color='red')\nplt.xticks([0,1,2], ['BG','OD','OC'])\nplt.yticks([0,1,2], ['BG','OD','OC'])\nplt.show()\n\n# --- Show a few example episodes ---\nfor _ in range(5):\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT+1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img_pil = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask_arr = load_mask(mask_map[key])\n        s_imgs.append(tf_img(img_pil))\n        s_msks.append(torch.from_numpy(mask_arr).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB').resize((IMG_SIZE,IMG_SIZE))\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_img(q_pil).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n        logits = logits[:, [2,1,0], :, :]\n        pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    fig, ax = plt.subplots(1,3,figsize=(12,4))\n    ax[0].imshow(q_pil);                   ax[0].set_title(\"Query\");   ax[0].axis('off')\n    ax[1].imshow(color_mask(q_gt));        ax[1].set_title(\"GT Mask\"); ax[1].axis('off')\n    ax[2].imshow(color_mask(pred));        ax[2].set_title(\"Pred Mask\");ax[2].axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:47:02.102527Z","iopub.execute_input":"2025-05-03T09:47:02.102833Z","iopub.status.idle":"2025-05-03T09:47:27.169428Z","shell.execute_reply.started":"2025-05-03T09:47:02.10281Z","shell.execute_reply":"2025-05-03T09:47:27.1686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport kornia  # GPU‐accelerated image ops\n\n# re‐use your Clahe module\nclass Clahe(torch.nn.Module):\n    def __init__(self, clip_limit: float = 40.0, grid_size=(8, 8)):\n        super().__init__()\n        self.clip_limit = float(clip_limit)\n        self.grid_size = grid_size\n\n    def forward(self, img: torch.Tensor) -> torch.Tensor:\n        batch = img.unsqueeze(0)\n        eq = kornia.enhance.equalize_clahe(\n            batch,\n            clip_limit=self.clip_limit,\n            grid_size=self.grid_size\n        )\n        return eq.squeeze(0)\n\n# --- Color mask for visualization ---\ndef color_mask(mask):\n    h,w = mask.shape\n    col = np.zeros((h,w,3),dtype=float)\n    col[mask==0] = [1,1,0]   # BG  → yellow\n    col[mask==1] = [0,1,0]   # OD  → green\n    col[mask==2] = [1,0,0]   # OC  → red\n    return col\n\n# --- Paths & params ---\nORIGA_IMG_DIR  = '/kaggle/input/glaucoma-datasets/ORIGA/Images'\nORIGA_MSK_DIR  = '/kaggle/input/glaucoma-datasets/ORIGA/Masks'\nMODEL_PATH     = '/kaggle/working/model_final.pth'\nIMG_SIZE       = 128\nNUM_SUPPORT    = 3\nNUM_EPISODES   = 100\nDEVICE         = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Transforms (with CLAHE) ---\ntf_img = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    Clahe(clip_limit=40.0, grid_size=(8,8)),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- Load mask (0,1,2 encoding) ---\ndef load_mask(path):\n    m = Image.open(path).convert('L')\n    m = m.resize((IMG_SIZE,IMG_SIZE), Image.NEAREST)\n    return np.array(m, dtype=int)\n\n# --- Gather file lists ---\nimg_files = sorted([\n    os.path.join(ORIGA_IMG_DIR,f)\n    for f in os.listdir(ORIGA_IMG_DIR) if f.lower().endswith('.jpg')\n])\nmask_map = {\n    os.path.splitext(f)[0]: os.path.join(ORIGA_MSK_DIR,f)\n    for f in os.listdir(ORIGA_MSK_DIR) if f.lower().endswith('.png')\n}\n\n# --- Load model ---\nmodel = ProtoAttentionUNet(in_ch=3, num_classes=3, feat_dim=256).to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\n\n# --- Dice metric ---\ndef dice_score(pred, gt, eps=1e-6):\n    scores = []\n    for c in range(3):\n        p = (pred==c)\n        g = (gt  ==c)\n        inter = (p & g).sum()\n        union = p.sum() + g.sum()\n        scores.append((2*inter+eps)/(union+eps))\n    return np.array(scores)\n\n# --- Confusion matrix accumulator ---\nconf_mat = np.zeros((3,3), dtype=int)\nall_scores = []\n\n# --- Meta-test episodes ---\nfor _ in tqdm(range(NUM_EPISODES), desc='Meta-Test'):\n    # sample support + query\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT+1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    # build support tensors\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img_pil = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask_arr = load_mask(mask_map[key])\n        s_imgs.append(tf_img(img_pil))\n        s_msks.append(torch.from_numpy(mask_arr).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    # load query\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB')\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_img(q_pil).unsqueeze(0).to(DEVICE)\n\n    # inference + channel‐swap fix\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n        logits = logits[:, [2,1,0], :, :]\n        pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    # record dice\n    sc = dice_score(pred, q_gt)\n    all_scores.append(sc)\n\n    # update confusion\n    for i in range(3):\n        for j in range(3):\n            conf_mat[i,j] += int(((q_gt==i) & (pred==j)).sum())\n\n# --- Report results ---\nall_scores = np.stack(all_scores)\nmean_dice = all_scores.mean(axis=0)\nprint(\"Mean per-class Dice:\")\nprint(f\"  BG: {mean_dice[0]:.4f}, OD: {mean_dice[1]:.4f}, OC: {mean_dice[2]:.4f}\")\nprint(f\"Overall Mean Dice: {mean_dice.mean():.4f}\\n\")\n\nprint(\"Confusion Matrix (rows=GT, cols=Pred):\")\nprint(conf_mat)\n\n# --- Plot confusion matrix ---\nplt.figure(figsize=(5,5))\nplt.imshow(conf_mat, cmap='Blues')\nplt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Ground Truth\")\nfor i in range(3):\n    for j in range(3):\n        plt.text(j, i, conf_mat[i,j], ha='center', va='center', color='red')\nplt.xticks([0,1,2], ['BG','OD','OC'])\nplt.yticks([0,1,2], ['BG','OD','OC'])\nplt.show()\n\n# --- Show a few example episodes ---\nfor _ in range(5):\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT+1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img_pil = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask_arr = load_mask(mask_map[key])\n        s_imgs.append(tf_img(img_pil))\n        s_msks.append(torch.from_numpy(mask_arr).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB').resize((IMG_SIZE,IMG_SIZE))\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_img(q_pil).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n        logits = logits[:, [2,1,0], :, :]\n        pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    fig, ax = plt.subplots(1,3,figsize=(12,4))\n    ax[0].imshow(q_pil);                   ax[0].set_title(\"Query\");   ax[0].axis('off')\n    ax[1].imshow(color_mask(q_gt));        ax[1].set_title(\"GT Mask\"); ax[1].axis('off')\n    ax[2].imshow(color_mask(pred));        ax[2].set_title(\"Pred Mask\");ax[2].axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:53:59.967423Z","iopub.execute_input":"2025-05-03T09:53:59.96773Z","execution_failed":"2025-05-03T09:57:30.589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport kornia  # GPU‐accelerated image ops\n\n# re‐use your Clahe module\nclass Clahe(torch.nn.Module):\n    def __init__(self, clip_limit: float = 40.0, grid_size=(8, 8)):\n        super().__init__()\n        self.clip_limit = float(clip_limit)\n        self.grid_size = grid_size\n\n    def forward(self, img: torch.Tensor) -> torch.Tensor:\n        batch = img.unsqueeze(0)\n        eq = kornia.enhance.equalize_clahe(\n            batch,\n            clip_limit=self.clip_limit,\n            grid_size=self.grid_size\n        )\n        return eq.squeeze(0)\n\n# --- Color mask for visualization ---\ndef color_mask(mask):\n    h,w = mask.shape\n    col = np.zeros((h,w,3),dtype=float)\n    col[mask==0] = [1,1,0]   # BG  → yellow\n    col[mask==1] = [0,1,0]   # OD  → green\n    col[mask==2] = [1,0,0]   # OC  → red\n    return col\n\n# --- Paths & params ---\nORIGA_IMG_DIR  = '/kaggle/input/glaucoma-datasets/G1020/Images'\nORIGA_MSK_DIR  = '/kaggle/input/glaucoma-datasets/G1020/Masks'\nMODEL_PATH     = '/kaggle/working/model_final.pth'\nIMG_SIZE       = 128\nNUM_SUPPORT    = 3\nNUM_EPISODES   = 100\nDEVICE         = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Transforms (with CLAHE) ---\ntf_img = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    Clahe(clip_limit=40.0, grid_size=(8,8)),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- Load mask (0,1,2 encoding) ---\ndef load_mask(path):\n    m = Image.open(path).convert('L')\n    m = m.resize((IMG_SIZE,IMG_SIZE), Image.NEAREST)\n    return np.array(m, dtype=int)\n\n# --- Gather file lists ---\nimg_files = sorted([\n    os.path.join(ORIGA_IMG_DIR,f)\n    for f in os.listdir(ORIGA_IMG_DIR) if f.lower().endswith('.jpg')\n])\nmask_map = {\n    os.path.splitext(f)[0]: os.path.join(ORIGA_MSK_DIR,f)\n    for f in os.listdir(ORIGA_MSK_DIR) if f.lower().endswith('.png')\n}\n\n# --- Load model ---\nmodel = ProtoAttentionUNet(in_ch=3, num_classes=3, feat_dim=256).to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\n\n# --- Dice metric ---\ndef dice_score(pred, gt, eps=1e-6):\n    scores = []\n    for c in range(3):\n        p = (pred==c)\n        g = (gt  ==c)\n        inter = (p & g).sum()\n        union = p.sum() + g.sum()\n        scores.append((2*inter+eps)/(union+eps))\n    return np.array(scores)\n\n# --- Confusion matrix accumulator ---\nconf_mat = np.zeros((3,3), dtype=int)\nall_scores = []\n\n# --- Meta-test episodes ---\nfor _ in tqdm(range(NUM_EPISODES), desc='Meta-Test'):\n    # sample support + query\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT+1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    # build support tensors\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img_pil = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask_arr = load_mask(mask_map[key])\n        s_imgs.append(tf_img(img_pil))\n        s_msks.append(torch.from_numpy(mask_arr).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    # load query\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB')\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_img(q_pil).unsqueeze(0).to(DEVICE)\n\n    # inference + channel‐swap fix\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n        logits = logits[:, [2,1,0], :, :]\n        pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    # record dice\n    sc = dice_score(pred, q_gt)\n    all_scores.append(sc)\n\n    # update confusion\n    for i in range(3):\n        for j in range(3):\n            conf_mat[i,j] += int(((q_gt==i) & (pred==j)).sum())\n\n# --- Report results ---\nall_scores = np.stack(all_scores)\nmean_dice = all_scores.mean(axis=0)\nprint(\"Mean per-class Dice:\")\nprint(f\"  BG: {mean_dice[0]:.4f}, OD: {mean_dice[1]:.4f}, OC: {mean_dice[2]:.4f}\")\nprint(f\"Overall Mean Dice: {mean_dice.mean():.4f}\\n\")\n\nprint(\"Confusion Matrix (rows=GT, cols=Pred):\")\nprint(conf_mat)\n\n# --- Plot confusion matrix ---\nplt.figure(figsize=(5,5))\nplt.imshow(conf_mat, cmap='Blues')\nplt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Ground Truth\")\nfor i in range(3):\n    for j in range(3):\n        plt.text(j, i, conf_mat[i,j], ha='center', va='center', color='red')\nplt.xticks([0,1,2], ['BG','OD','OC'])\nplt.yticks([0,1,2], ['BG','OD','OC'])\nplt.show()\n\n# --- Show a few example episodes ---\nfor _ in range(5):\n    idxs = random.sample(range(len(img_files)), NUM_SUPPORT+1)\n    sup_idxs, qry_idx = idxs[:NUM_SUPPORT], idxs[-1]\n\n    s_imgs, s_msks = [], []\n    for i in sup_idxs:\n        img_pil = Image.open(img_files[i]).convert('RGB')\n        key = os.path.splitext(os.path.basename(img_files[i]))[0]\n        mask_arr = load_mask(mask_map[key])\n        s_imgs.append(tf_img(img_pil))\n        s_msks.append(torch.from_numpy(mask_arr).long())\n    s_imgs = torch.stack(s_imgs).to(DEVICE)\n    s_msks = torch.stack(s_msks).to(DEVICE)\n\n    q_pil = Image.open(img_files[qry_idx]).convert('RGB').resize((IMG_SIZE,IMG_SIZE))\n    key = os.path.splitext(os.path.basename(img_files[qry_idx]))[0]\n    q_gt = load_mask(mask_map[key])\n    q_tensor = tf_img(q_pil).unsqueeze(0).to(DEVICE)\n\n    with torch.no_grad():\n        logits, _ = model(s_imgs, s_msks, q_tensor)\n        logits = logits[:, [2,1,0], :, :]\n        pred = logits.argmax(1).squeeze(0).cpu().numpy()\n\n    fig, ax = plt.subplots(1,3,figsize=(12,4))\n    ax[0].imshow(q_pil);                   ax[0].set_title(\"Query\");   ax[0].axis('off')\n    ax[1].imshow(color_mask(q_gt));        ax[1].set_title(\"GT Mask\"); ax[1].axis('off')\n    ax[2].imshow(color_mask(pred));        ax[2].set_title(\"Pred Mask\");ax[2].axis('off')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}